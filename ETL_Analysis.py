# -*- coding: utf-8 -*-
# <nbformat>4.2</nbformat>

# ## Proyecto de Data Engineering: ETL y An√°lisis de Ventas (Microsoft Fabric / PySpark)
# 
# # Script COMPLETO y DEFINITIVO: Resuelve problemas de integridad de datos (nulidad y unicidad) y genera la r√©plica de visualizaciones est√°ticas.

import re
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.sql.functions import col, to_date, date_format, year, month, dayofmonth, lit, when, first, regexp_replace

# Nota: Asume que Spark est√° pre-inicializado (como en Fabric o Databricks).

# --- 1. CONFIGURACI√ìN Y LECTURA DEL ARCHIVO CSV ---
# üö® RUTA CORREGIDA: Usando el enlace RAW de GitHub para asegurar la portabilidad.
# (He reemplazado el hash largo del commit por 'main' o 'master' para una URL m√°s estable en la rama principal)
RAW_FILE_PATH = "https://raw.githubusercontent.com/Nicolenki7/Tech_GlobalStore_Retail_Analysis-Fabric/main/train.csv" 

# Lectura del CSV
df_raw = spark.read.csv(RAW_FILE_PATH, header=True, inferSchema=True)
print("‚úÖ 1. Datos crudos le√≠dos exitosamente desde GitHub.")

# --- 2. LIMPIEZA DE NOMBRES DE COLUMNA Y CONVERSI√ìN DE TIPOS ---
df_clean = df_raw
# Estandarizaci√≥n de nombres:
for column in df_clean.columns:
    new_column = re.sub(r'[\s-]', '_', column) 
    df_clean = df_clean.withColumnRenamed(column, new_column)

# Conversi√≥n de tipos de datos cruciales
df_clean = df_clean.withColumn("Sales", col("Sales").cast("double")) \
                 .withColumn("Order_Date", to_date(col("Order_Date"), "MM/dd/yyyy")) \
                 .withColumn("Ship_Date", to_date(col("Ship_Date"), "MM/dd/yyyy")) \
                 .withColumn("Postal_Code", col("Postal_Code").cast("string")) \
                 .withColumn("Row_ID", col("Row_ID").cast("long"))

print("‚úÖ 2. Tipos de datos estandarizados y nombres de columnas corregidos.")

# --- 3. CREACI√ìN DE TABLAS DE DIMENSI√ìN (CORRECCI√ìN DE INTEGRIDAD) ---

# üõë DIMENSI√ìN PRODUCTO (Dim_Product) - SOLUCI√ìN A DUPLICIDAD
# Agrupamos por Product_ID para garantizar la unicidad de la clave.
df_dim_product = df_clean.groupBy("Product_ID").agg(
    first(col("Category")).alias("Category"),
    first(col("Sub_Category")).alias("Sub_Category"),
    first(col("Product_Name")).alias("Product_Name")
)
print("‚úÖ Dim_Product corregida.")


# DIMENSI√ìN: CLIENTE (Dim_customer)
df_dim_customer = df_clean.selectExpr(
    "Customer_ID",
    "Customer_Name",
    "Segment"
).distinct()


# üõë DIMENSI√ìN: GEOGRAF√çA (Dim_Geography) - SOLUCI√ìN A NULOS Y DUPLICIDAD
# 1. Filtramos los nulos en Postal_Code.
# 2. Agrupamos por Postal_Code para garantizar la unicidad.
df_dim_geography = df_clean.filter(col("Postal_Code").isNotNull()) \
                           .groupBy("Postal_Code").agg(
    first(col("Country")).alias("Country"),
    first(col("City")).alias("City"),
    first(col("State")).alias("State"),
    first(col("Region")).alias("Region")
)
print("‚úÖ Dim_Geography corregida.")


# üõë DIMENSI√ìN: FECHAS (Dim_Date) - SOLUCI√ìN A NULOS
# 1. Filtramos los nulos en Order_Date.
Max_A√±o_Datos = df_clean.select(year(col("Order_Date"))).agg({"year(Order_Date)": "max"}).collect()[0][0]

df_dim_date = (
    df_clean.filter(col("Order_Date").isNotNull()) 
    .select(col("Order_Date").alias("Date"))
    .distinct()
    .withColumn("A√±o", year(col("Date")))
    .withColumn("Nombre_del_mes", date_format(col("Date"), "MMMM"))
    .withColumn("Trimestre", when(month(col("Date")) <= 3, 1)
                           .when(month(col("Date")) <= 6, 2)
                           .when(month(col("Date")) <= 9, 3)
                           .otherwise(4))
    .withColumn("D√≠a_de_la_semana", date_format(col("Date"), "EEEE"))
    .withColumn("Es_√öltimo_A√±o", when(year(col("Date")) == Max_A√±o_Datos, lit(True)).otherwise(lit(False)))
    .withColumnRenamed("Nombre_del_mes", "Nombre_del_mes")
)
print("‚úÖ Dim_Date corregida.")


# --- 4. CREACI√ìN DE LA TABLA DE HECHOS (Fact_Sales) ---
df_fact_sales = df_clean.selectExpr(
    "Row_ID",
    "Order_ID",
    "Order_Date",
    "Ship_Date",
    "Ship_Mode",
    "Customer_ID",
    "Product_ID",
    "Sales",
    "Postal_Code"
)

print("‚úÖ 3. Tablas de Hechos y Dimensiones creadas.")

# --- 5. ESCRITURA FINAL COMO TABLAS DELTA (Para Fabric/Databricks) ---
# Se mantiene el c√≥digo de escritura para uso en entornos de Lakehouse.
df_fact_sales.write.mode("overwrite").format("delta").option("overwriteSchema", "true").save("Tables/Fact_sales")
df_dim_product.write.mode("overwrite").format("delta").option("overwriteSchema", "true").save("Tables/Dim_Product")
df_dim_customer.write.mode("overwrite").format("delta").save("Tables/Dim_customer")
df_dim_geography.write.mode("overwrite").format("delta").option("overwriteSchema", "true").save("Tables/Dim_Geography")
df_dim_date.write.mode("overwrite").format("delta").option("overwriteSchema", "true").save("Tables/Dim_Date")

print("üéâ 4. ¬°Proceso ETL completado! Tablas Delta escritas/sobrescritas.")

# =========================================================================
# --- 6. VISUALIZACI√ìN DE R√âPLICA (Matplotlib/Seaborn) ---
# =========================================================================

print("\n--- Generando Replicas de Visualizaci√≥n Est√°tica (Matplotlib/Seaborn) ---")

# Convertimos la tabla limpia principal a Pandas para el an√°lisis y visualizaci√≥n local
# Usamos un muestreo si el dataset es muy grande, pero aqu√≠ usamos toPandas() completo.
df_pd = df_clean.toPandas()

plt.style.use('seaborn-v0_8-whitegrid')
fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 18))
plt.subplots_adjust(hspace=0.5)

# --- Gr√°fico 1: Sales by Product Category (Gr√°fico de Barras) ---
sales_by_category = df_pd.groupby('Category')['Sales'].sum().sort_values(ascending=False)
sns.barplot(x=sales_by_category.index, y=sales_by_category.values, ax=axes[0], palette="Blues_d")
axes[0].set_title('Sales by Product Category', fontsize=16)
axes[0].set_xlabel('Category')
axes[0].set_ylabel('Total Sales (USD)')
axes[0].ticklabel_format(style='plain', axis='y')

# --- Gr√°fico 2: Historical Sales Trend (Gr√°fico de L√≠neas) ---
# Agregaci√≥n por Mes/A√±o
df_pd['YearMonth'] = df_pd['Order_Date'].dt.to_period('M')
sales_trend = df_pd.groupby('YearMonth')['Sales'].sum()
sales_trend.index = sales_trend.index.astype(str)

sns.lineplot(x=sales_trend.index, y=sales_trend.values, ax=axes[1], color='coral')
axes[1].set_title('Historical Sales Trend (Monthly)', fontsize=16)
axes[1].set_xlabel('Time (Year-Month)')
axes[1].set_ylabel('Total Sales (USD)')
axes[1].tick_params(axis='x', rotation=45, labelsize=8)
axes[1].ticklabel_format(style='plain', axis='y') 
axes[1].locator_params(axis='x', nbins=10)

# --- Gr√°fico 3: Customer Value and Frequency Analysis (Scatter Plot) ---
customer_sales = df_pd.groupby('Customer_ID')['Sales'].sum().reset_index(name='Total_Sales')
customer_orders = df_pd.groupby('Customer_ID')['Order_ID'].nunique().reset_index(name='Total_Orders')

df_customer_analysis = pd.merge(customer_sales, customer_orders, on='Customer_ID')

sns.scatterplot(
    x='Total_Orders', 
    y='Total_Sales', 
    data=df_customer_analysis, 
    ax=axes[2], 
    alpha=0.6,
    color='#4CAF50'
)
axes[2].set_title('Customer Value and Frequency Analysis', fontsize=16)
axes[2].set_xlabel('Total Orders per Customer (Frequency)')
axes[2].set_ylabel('Total Sales per Customer (Value)')

plt.show()
print("üéâ Visualizaciones est√°ticas generadas exitosamente.")
